"""
CyberDailyWatch - Scraper d'actualit√©s
Module de r√©cup√©ration des actualit√©s cybers√©curit√© depuis TheHackerNews.

Ce module fournit des fonctions pour scraper les derniers articles
de TheHackerNews.com et extraire les titres, URLs et r√©sum√©s.

Configuration modifiable:
    - URL_SOURCE: URL du site √† scraper (ligne 16)
    - NUM_ARTICLES: Nombre d'articles par d√©faut (param√®tre de fonction)
    - USER_AGENT: Agent utilisateur pour les requ√™tes (ligne 23)
"""

import requests
from bs4 import BeautifulSoup
from typing import List, Dict

# =============================================================================
# CONFIGURATION - Modifiez ces valeurs selon vos besoins
# =============================================================================

# URL du site source des actualit√©s
URL_SOURCE = "https://thehackernews.com"

# En-t√™tes HTTP pour simuler un navigateur classique
# Modifiez si vous rencontrez des blocages
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# D√©lai d'attente maximal pour les requ√™tes (en secondes)
TIMEOUT = 15


def scrape_hackernews(num_articles: int = 3) -> List[Dict[str, str]]:
    """
    R√©cup√®re les derni√®res actualit√©s de TheHackerNews.com.
    
    Cette fonction effectue une requ√™te HTTP vers le site, parse le HTML
    et extrait les informations des articles.
    
    Args:
        num_articles: Nombre d'articles √† r√©cup√©rer (d√©faut: 3)
                      Modifiez cette valeur pour obtenir plus/moins d'articles
    
    Returns:
        Liste de dictionnaires contenant pour chaque article:
        - title: Titre de l'article (en anglais)
        - url: Lien vers l'article complet
        - summary: R√©sum√©/extrait de l'article
    
    Raises:
        Ne l√®ve pas d'exception, retourne une liste vide en cas d'erreur
    
    Exemple d'utilisation:
        >>> articles = scrape_hackernews(5)  # R√©cup√®re 5 articles
        >>> for article in articles:
        ...     print(article['title'])
    """
    try:
        # Effectuer la requ√™te HTTP
        response = requests.get(URL_SOURCE, headers=HEADERS, timeout=TIMEOUT)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"‚ùå Erreur lors de la r√©cup√©ration de {URL_SOURCE}: {e}")
        return []
    
    # Parser le HTML avec BeautifulSoup
    soup = BeautifulSoup(response.text, "html.parser")
    articles = []
    
    # Rechercher les √©l√©ments d'articles
    # Structure HTML de TheHackerNews: <a class="story-link"> contient le lien et titre
    story_links = soup.find_all("a", class_="story-link")
    
    for link in story_links[:num_articles]:
        try:
            # Extraire l'URL de l'article
            article_url = link.get("href", "")
            
            # Extraire le titre depuis l'√©l√©ment h2
            title_element = link.find("h2", class_="home-title")
            if not title_element:
                continue
            
            title = title_element.get_text(strip=True)
            
            # Extraire le r√©sum√© depuis le parent (div.body-post)
            parent = link.find_parent("div", class_="body-post")
            summary = ""
            if parent:
                excerpt_element = parent.find("div", class_="home-desc")
                summary = excerpt_element.get_text(strip=True) if excerpt_element else ""
            
            # Ajouter l'article si les donn√©es essentielles sont pr√©sentes
            if title and article_url:
                articles.append({
                    "title": title,
                    "url": article_url,
                    "summary": summary
                })
                
        except Exception as e:
            # Continuer avec les autres articles en cas d'erreur
            print(f"‚ö†Ô∏è Erreur lors du parsing d'un article: {e}")
            continue
    
    return articles


# =============================================================================
# POINT D'ENTR√âE - Test direct du module
# =============================================================================
if __name__ == "__main__":
    import json
    
    print("üîç Test du scraper TheHackerNews...")
    print(f"üì° Source: {URL_SOURCE}")
    print()
    
    # R√©cup√©rer les articles
    news = scrape_hackernews(3)
    
    if news:
        print(f"‚úÖ {len(news)} articles r√©cup√©r√©s:\n")
        print(json.dumps(news, indent=2, ensure_ascii=False))
    else:
        print("‚ùå Aucun article trouv√©")
        print("üí° V√©rifiez votre connexion internet ou la structure du site")
